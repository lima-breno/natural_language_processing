{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lima-breno/natural_language_processing/blob/main/Classificador_(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classificação de Documentos\n",
        "\n",
        "A clssificação de documentos é muito útil em vários aspectos. Um dos tipos de classificação de texto é a análise de sentimentos.\n",
        "\n",
        "A fim de ilustrar a classificação de documentos iremos criar um modelo para classificar uma frase como positiva ou negativa.\n",
        "\n",
        "## Carregando o embedding e os dados"
      ],
      "metadata": {
        "id": "J6e5fLiZS8nE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unidecode\n",
        "!pip install vaderSentiment"
      ],
      "metadata": {
        "id": "KXAMuXYrW06o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9281f14-1644-448c-e25c-be72be130cd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.4.0-py3-none-any.whl.metadata (13 kB)\n",
            "Downloading Unidecode-1.4.0-py3-none-any.whl (235 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/235.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.8/235.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.4.0\n",
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from vaderSentiment) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (2025.4.26)\n",
            "Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from unidecode import unidecode\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re"
      ],
      "metadata": {
        "id": "Gi9S_uWeWCZs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "729db5a0-1a5f-464c-e07b-6325a9d6222b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'gensim'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-1461145172>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0munidecode\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0munidecode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#opção 1 -> montar o drive no colab e acessar o arquivo de embedding do drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "metadata": {
        "id": "kwtuxwvYS8Uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "path='drive/MyDrive/aulas/Processamento de Linguagem Natural/ptwiki_20180420_100d.txt.bz2'\n",
        "word_vectors = gensim.models.KeyedVectors.load_word2vec_format(path, binary=False)"
      ],
      "metadata": {
        "id": "6_obUVTjV7G5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tratamento dos dados\n",
        "\n",
        "1. Iremos tornar a nossa variável alvo como binaria\n",
        "2. Iremos criar um pré-processamento\n",
        "3. Iremos criar a representação textual"
      ],
      "metadata": {
        "id": "HLLT7T0iTu96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('drive/MyDrive/aulas/Processamento de Linguagem Natural/imdb-reviews-pt-br.csv')"
      ],
      "metadata": {
        "id": "Or7JkHeCWRnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "1gb0tMr-2wnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPujV1LnOXmW"
      },
      "outputs": [],
      "source": [
        "target = df['sentiment'].replace(['neg','pos'],[0,1])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pre_processamento_texto_return_str(corpus, portugues_stops):\n",
        "    corpus_alt = re.findall(r\"\\w+(?:'\\w+)?|[^\\w\\s]\", corpus) # extração palavras -> palavras\n",
        "    corpus_alt = [t.lower() for t in corpus_alt] #passando para minusculo\n",
        "    corpus_alt = [t for t in corpus_alt if t not in portugues_stops] #remoção dos tokens que são SW\n",
        "    corpus_alt = [t for t in corpus_alt if t not in string.punctuation] # o mesmo para pontuação\n",
        "    corpus_alt = [re.sub(r'\\d', '', t) for t in corpus_alt] #remoção de numeros\n",
        "    corpus_alt_str = ' '.join(corpus_alt)\n",
        "    return corpus_alt_str"
      ],
      "metadata": {
        "id": "u_niZrk8T_GJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pre_processamento_texto_return_token(corpus, portugues_stops):\n",
        "    corpus_alt = re.findall(r\"\\w+(?:'\\w+)?|[^\\w\\s]\", corpus)\n",
        "    corpus_alt = [t.lower() for t in corpus_alt]\n",
        "    portugues_stops = stopwords.words('portuguese')\n",
        "    corpus_alt = [t for t in corpus_alt if t not in portugues_stops]\n",
        "    corpus_alt = [t for t in corpus_alt if t not in string.punctuation]\n",
        "    corpus_alt = [re.sub(r'\\d', '', t) for t in corpus_alt]\n",
        "\n",
        "    return corpus_alt"
      ],
      "metadata": {
        "id": "fshMqXT7UcbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "1hXgVIHFmJZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "portugues_stops = stopwords.words('portuguese')"
      ],
      "metadata": {
        "id": "PKCg3y_QUDm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "df[\"text_pt_sem_stopwords\"] = df[\"text_pt\"]\\\n",
        "      .progress_apply(lambda x: pre_processamento_texto_return_str(x, portugues_stops))"
      ],
      "metadata": {
        "id": "0UcU8WG8UAIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "1__HLCFI3Xaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bag-of-word"
      ],
      "metadata": {
        "id": "kduTo4PrUHR6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vect_bag = CountVectorizer()\n",
        "X_bag = vect_bag.fit_transform(df['text_pt_sem_stopwords'])\n",
        "vocabulario = vect_bag.get_feature_names_out()"
      ],
      "metadata": {
        "id": "GsuuloJHUKSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Vocabulario\",len(vocabulario))\n",
        "print(\"Features\",X_bag.shape)\n",
        "print(\"Target\",target.shape)"
      ],
      "metadata": {
        "id": "k-59CVJ6UTnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding"
      ],
      "metadata": {
        "id": "hv9BDAW1UZyx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"text_pt_sem_stopwords_token\"] = df[\"text_pt\"]\\\n",
        ".progress_apply(lambda x: pre_processamento_texto_return_token(x, portugues_stops))"
      ],
      "metadata": {
        "id": "u2G8H45zUZUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def calcula_embedding_frase(tokens):\n",
        "    return np.mean([word_vectors[t] for t in tokens if t in word_vectors.key_to_index.keys()], axis=0)"
      ],
      "metadata": {
        "id": "mrPkV_qQUvUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_embedding = df[\"text_pt_sem_stopwords_token\"].progress_apply(lambda x: calcula_embedding_frase(x))"
      ],
      "metadata": {
        "id": "SUIDpiIXUxA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_embedding"
      ],
      "metadata": {
        "id": "L8B7xbUiRGK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Treinamento"
      ],
      "metadata": {
        "id": "FAAjfdIiU54d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_bow, X_test_bow, y_train_bow, y_test_bow = \\\n",
        "    train_test_split(X_bag, target, random_state=123)"
      ],
      "metadata": {
        "id": "lCoWK2wvU8A9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelo_bow = LogisticRegression()\n",
        "modelo_bow.fit(X_train_bow,y_train_bow)"
      ],
      "metadata": {
        "id": "Pu926CBBU8rV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = modelo_bow.predict(X_test_bow)"
      ],
      "metadata": {
        "id": "t2wZ4vR5VAGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test_bow, y_pred))"
      ],
      "metadata": {
        "id": "kcrdBikGVBTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding"
      ],
      "metadata": {
        "id": "lCCLnh-6VDCX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_embedding, X_test_embedding, y_train_embedding, y_test_embedding = \\\n",
        "train_test_split(X_embedding.values, target,random_state=123)"
      ],
      "metadata": {
        "id": "ShvzGLQhVFTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_embedding = pd.DataFrame([x for x in X_train_embedding])\n",
        "X_test_embedding = pd.DataFrame([x for x in X_test_embedding])"
      ],
      "metadata": {
        "id": "4qpSVV1GVH1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_embedding.head()"
      ],
      "metadata": {
        "id": "ErW51fziRrWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelo_embedding = LogisticRegression()\n",
        "modelo_embedding.fit(X_train_embedding,y_train_embedding)"
      ],
      "metadata": {
        "id": "fl3mu4VMVM10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = modelo_embedding.predict(X_test_embedding)"
      ],
      "metadata": {
        "id": "8bfYfWZTVOlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test_embedding, y_pred))"
      ],
      "metadata": {
        "id": "O4cBguOlVPrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Análise de sentimentos\n",
        "\n",
        "Quando o objetivo é realizar análise de sentimentos podemos treinar o nosso proprio modelo ou utilizar ferramentas já feitas. Exemplo: Vader.\n",
        "\n",
        "O VADER (Valence Aware Dictionary e sEntiment Reasoner) é uma ferramenta de análise de sentimentos baseada em regras e léxico, especificamente identifica os sentimentos expressos nas mídias sociais.\n",
        "\n",
        "- positive sentiment: compound score >= 0.05\n",
        "- neutral sentiment: (compound score > -0.05) e (compound score < 0.05)\n",
        "- negative sentiment: compound score <= -0.05\n",
        "\n",
        "Mais informações: https://github.com/cjhutto/vaderSentiment"
      ],
      "metadata": {
        "id": "AKNj35JXVY34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
      ],
      "metadata": {
        "id": "-FtJR2x4Voit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyzer = SentimentIntensityAnalyzer()"
      ],
      "metadata": {
        "id": "9wIOnjcrVqgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texto_neg = df.loc[0, \"text_en\"]\n",
        "texto_pos = df.loc[49431, \"text_en\"]"
      ],
      "metadata": {
        "id": "SWVKxQFRVsQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyzer.polarity_scores(texto_neg)"
      ],
      "metadata": {
        "id": "bPuneH6MVuXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyzer.polarity_scores(df.loc[2, \"text_en\"])"
      ],
      "metadata": {
        "id": "OpguZ-Py43Gc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyzer.polarity_scores(texto_pos)"
      ],
      "metadata": {
        "id": "3vdErQguVuQR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
