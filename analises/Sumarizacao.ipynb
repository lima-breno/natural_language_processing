{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lima-breno/natural_language_processing/blob/main/Sumarizacao.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QiuQd1uqbmm"
      },
      "outputs": [],
      "source": [
        "import yaml\n",
        "import re\n",
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import numpy as np\n",
        "import operator\n",
        "import networkx as nx\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sumarização\n",
        "\n",
        "##Coleta do dado"
      ],
      "metadata": {
        "id": "k6PgagwBIdjY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "noticia_url = \"https://g1.globo.com/tecnologia/noticia/2022/03/12/instagram-restrito-na-russia-entenda-a-importancia-da-rede-social-para-o-pais-de-putin.ghtml\""
      ],
      "metadata": {
        "id": "8IE0akLtIgmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def match_class(target):\n",
        "    def do_match(tag):\n",
        "        classes = tag.get('class', [])\n",
        "        return all(c in classes for c in target)\n",
        "    return do_match\n",
        "\n",
        "def get_text_url(url):\n",
        "    res = requests.get(url)\n",
        "    html = res.text\n",
        "    soup = BeautifulSoup(html, 'html5lib')\n",
        "    #remove marcações de scripts e style\n",
        "    texto = soup.find_all(match_class([\"content-text__container\"]))\n",
        "    all_text = \"\"\n",
        "    for t in texto:\n",
        "        all_text += t.get_text()\n",
        "    return all_text"
      ],
      "metadata": {
        "id": "NiuZRUNWIh5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texto_noticia = get_text_url(noticia_url)"
      ],
      "metadata": {
        "id": "Ki01cgiNIjed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texto_noticia"
      ],
      "metadata": {
        "id": "61jpj7wjrraJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pré-processamento"
      ],
      "metadata": {
        "id": "ST-zAOQzIs59"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(set(texto_noticia.split()))"
      ],
      "metadata": {
        "id": "qq4IsftQtmCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(texto_noticia)"
      ],
      "metadata": {
        "id": "k0lsQ_nHuIXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')\n",
        "corpus_sent = sent_tokenize(texto_noticia)"
      ],
      "metadata": {
        "id": "--G9Zkd4Il0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_sent"
      ],
      "metadata": {
        "id": "5ebKgfBpRoX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "kB56rnnU91yA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_sent"
      ],
      "metadata": {
        "id": "Flk88o5k11y2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pre_processamento_texto(corpus):\n",
        "    corpus_alt = re.findall(r\"\\w+(?:'\\w+)?|[^\\w\\s]\", corpus)\n",
        "    corpus_alt = [t.lower() for t in corpus_alt]\n",
        "    portugues_stops = stopwords.words('portuguese')\n",
        "    corpus_alt = [t for t in corpus_alt if t not in portugues_stops]\n",
        "    corpus_alt = [t for t in corpus_alt if t not in string.punctuation]\n",
        "\n",
        "    return corpus_alt"
      ],
      "metadata": {
        "id": "wTyzMY8P9tvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_processado = [pre_processamento_texto(sent) for sent in corpus_sent]"
      ],
      "metadata": {
        "id": "g-OPmg3gIolI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_processado[0]"
      ],
      "metadata": {
        "id": "1PiGkfOwte6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulario = set()\n",
        "for corpus in corpus_processado:\n",
        "    vocabulario.update(set(corpus))"
      ],
      "metadata": {
        "id": "G1awiwVtthKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocabulario)"
      ],
      "metadata": {
        "id": "J9TH1mvmt45A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para realizar a sumarização iremos criar os seguintes métodos;\n",
        "\n",
        "1. Um que calcula a similaridade de frases. Usando Bow\n",
        "2. Iremos construir uma matriz de similaridade\n",
        "3. Iremos fazer o rank das frases utilizando o método pagerank. Este método utiliza da representação de grafos então transformaremos nosso dado em um grafo.\n",
        "4. Ordene o score e retorne os 5 primeiros."
      ],
      "metadata": {
        "id": "vMQAJtGeIyLK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def similarity_sentences(sent1, sent2):\n",
        "    all_words = list(set(sent1 + sent2))\n",
        "    vect_bag = CountVectorizer(binary=False, analyzer=\"word\")\n",
        "    vect_bag.fit(all_words)\n",
        "    #aplica BOW\n",
        "    vec_sent1 = np.asarray(vect_bag.transform([' '.join(sent1)]).todense())\n",
        "    vec_sent2 = np.asarray(vect_bag.transform([' '.join(sent2)]).todense())\n",
        "\n",
        "    return cosine_similarity(vec_sent1, vec_sent2).reshape(-1)[0]"
      ],
      "metadata": {
        "id": "OZng8-wTJST3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def matrix_similarity(sentences):\n",
        "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
        "    for ix in range(0,len(sentences)):\n",
        "        for ix2 in range(0,len(sentences)):\n",
        "            if ix == ix2:\n",
        "                continue\n",
        "            similarity_matrix[ix][ix2] = similarity_sentences(sentences[ix], sentences[ix2])\n",
        "    return similarity_matrix"
      ],
      "metadata": {
        "id": "2zlfRiAmJTOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pagerank(texto):\n",
        "    sentences_matrix_similarity = matrix_similarity(texto)\n",
        "    sentence_similarity_graph = nx.from_numpy_array(sentences_matrix_similarity)\n",
        "    scores = nx.pagerank(sentence_similarity_graph)\n",
        "\n",
        "    return scores"
      ],
      "metadata": {
        "id": "snB0z9woJUYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize(scores):\n",
        "    scores_sorted = dict(sorted(scores.items(), key=operator.itemgetter(1),reverse=True))\n",
        "    rank_sentences = list(scores_sorted.keys())[:10]\n",
        "    summarize_text = \"\"\n",
        "    for r in rank_sentences:\n",
        "        summarize_text += (corpus_sent[r]) + \" \"\n",
        "    return summarize_text"
      ],
      "metadata": {
        "id": "PtNIg1cmJVNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarize(get_pagerank(corpus_processado))"
      ],
      "metadata": {
        "id": "frovH8fjJWuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Trocando para tfidf\n",
        "def similarity_sentences(sent1, sent2):\n",
        "    all_words = list(set(sent1 + sent2))\n",
        "    vect = TfidfVectorizer()\n",
        "\n",
        "    vect.fit(all_words)\n",
        "    vec_sent1 = np.asarray(vect.transform([' '.join(sent1)]).todense())\n",
        "    vec_sent2 = np.asarray(vect.transform([' '.join(sent2)]).todense())\n",
        "\n",
        "    return cosine_similarity(vec_sent1, vec_sent2).reshape(-1)[0]"
      ],
      "metadata": {
        "id": "TSnoAwv9JYeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarize(get_pagerank(corpus_processado))"
      ],
      "metadata": {
        "id": "S7yuOflqJd2V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
